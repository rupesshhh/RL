{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**NAME:** RUPESH DHIRWANI\n",
        "\n",
        "**ROLL NO:** 10\n",
        "\n",
        "**CLASS:** D16AD"
      ],
      "metadata": {
        "id": "l75mZgQ6GEbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class GridWorldMDP:\n",
        "    def __init__(self, num_rows, num_cols, terminal_states, rewards):\n",
        "        self.num_rows = num_rows\n",
        "        self.num_cols = num_cols\n",
        "        self.terminal_states = terminal_states\n",
        "        self.rewards = rewards\n",
        "        self.num_states = num_rows * num_cols\n",
        "        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # right, left, down, up\n",
        "        self.transition_prob = self._initialize_transition_prob()\n",
        "\n",
        "    def _initialize_transition_prob(self):\n",
        "        transition_prob = np.zeros((self.num_states, len(self.actions), self.num_states))\n",
        "        for state in range(self.num_states):\n",
        "            for action_index, action in enumerate(self.actions):\n",
        "                next_state = self._get_next_state(state, action)\n",
        "                transition_prob[state, action_index, next_state] = 1\n",
        "        return transition_prob\n",
        "\n",
        "    def _get_next_state(self, state, action):\n",
        "        row, col = divmod(state, self.num_cols)\n",
        "        next_row = max(0, min(row + action[0], self.num_rows - 1))\n",
        "        next_col = max(0, min(col + action[1], self.num_cols - 1))\n",
        "        next_state = next_row * self.num_cols + next_col\n",
        "        if (next_row, next_col) in self.terminal_states:\n",
        "            return next_state, self.rewards[(next_row, next_col)]\n",
        "        return next_state, 0\n",
        "\n",
        "    def policy_evaluation(self, policy, discount_factor=0.9, theta=0.0001):\n",
        "        V = np.zeros(self.num_states)\n",
        "        while True:\n",
        "            delta = 0\n",
        "            for state in range(self.num_states):\n",
        "                v = V[state]\n",
        "                new_v = 0\n",
        "                for action_index, action in enumerate(self.actions):\n",
        "                    next_state, reward = self._get_next_state(state, action)\n",
        "                    new_v += policy[state, action_index] * (reward + discount_factor * V[next_state])\n",
        "                V[state] = new_v\n",
        "                delta = max(delta, abs(v - V[state]))\n",
        "            if delta < theta:\n",
        "                break\n",
        "        return V\n",
        "\n",
        "    def policy_iteration(self, discount_factor=0.9):\n",
        "        policy = np.ones((self.num_states, len(self.actions))) / len(self.actions)\n",
        "        while True:\n",
        "            V = self.policy_evaluation(policy, discount_factor)\n",
        "            policy_stable = True\n",
        "            for state in range(self.num_states):\n",
        "                old_action = np.argmax(policy[state])\n",
        "                action_values = np.zeros(len(self.actions))\n",
        "                for action_index, action in enumerate(self.actions):\n",
        "                    next_state, reward = self._get_next_state(state, action)\n",
        "                    action_values[action_index] = reward + discount_factor * V[next_state]\n",
        "                best_action = np.argmax(action_values)\n",
        "                if old_action != best_action:\n",
        "                    policy_stable = False\n",
        "                policy[state] = np.eye(len(self.actions))[best_action]\n",
        "            if policy_stable:\n",
        "                break\n",
        "        return policy, V\n",
        "\n",
        "def main():\n",
        "    num_rows = 5\n",
        "    num_cols = 3\n",
        "    terminal_states = {(0, 0): 1, (4, 2): 1}\n",
        "    rewards = {(0, 0): 1, (4, 2): 1}\n",
        "    grid_world = GridWorldMDP(num_rows, num_cols, terminal_states, rewards)\n",
        "\n",
        "    # Policy iteration\n",
        "    optimal_policy, optimal_values = grid_world.policy_iteration()\n",
        "\n",
        "    # Displaying the optimal policy\n",
        "    actions_str = ['R', 'L', 'D', 'U']\n",
        "    for state in range(grid_world.num_states):\n",
        "        row = state // num_cols\n",
        "        col = state % num_cols\n",
        "        print(f\"State ({row}, {col}): Action {actions_str[np.argmax(optimal_policy[state])]} Optimal Value: {optimal_values[state]}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fz0F183iF5t3",
        "outputId": "15a065ea-29ea-427e-d3fa-9ea80effd116"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State (0, 0): Action L Optimal Value: 9.999153585021714\n",
            "State (0, 1): Action L Optimal Value: 9.999238226519543\n",
            "State (0, 2): Action L Optimal Value: 8.999314403867588\n",
            "State (1, 0): Action U Optimal Value: 9.999238226519543\n",
            "State (1, 1): Action L Optimal Value: 8.999314403867588\n",
            "State (1, 2): Action L Optimal Value: 8.09938296348083\n",
            "State (2, 0): Action U Optimal Value: 8.999314403867588\n",
            "State (2, 1): Action L Optimal Value: 8.09938296348083\n",
            "State (2, 2): Action D Optimal Value: 8.999153585021714\n",
            "State (3, 0): Action U Optimal Value: 8.09938296348083\n",
            "State (3, 1): Action R Optimal Value: 8.999153585021714\n",
            "State (3, 2): Action D Optimal Value: 9.999153585021714\n",
            "State (4, 0): Action R Optimal Value: 8.999153585021714\n",
            "State (4, 1): Action R Optimal Value: 9.999153585021714\n",
            "State (4, 2): Action R Optimal Value: 9.999153585021714\n"
          ]
        }
      ]
    }
  ]
}