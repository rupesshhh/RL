{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**NAME:** RUPESH DHIRWANI\n",
        "\n",
        "**CLASS:** D16AD\n",
        "\n",
        "**ROLL NO:** 10"
      ],
      "metadata": {
        "id": "459EUlUwq2W1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBxb7N7LQGAH",
        "outputId": "4a80ae8c-b4c7-4f53-a105-2fc5e2a919f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class AntWorldEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        self.action_space = spaces.Discrete(4)  # Four possible actions: up, down, left, right\n",
        "        self.observation_space = spaces.Discrete(25)  # 5x5 grid world\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.grid_size = 5\n",
        "        self.ant_position = [0, 0]  # Initial position of the ant\n",
        "        self.goal_position = [4, 4]  # Goal position\n",
        "        self.done = False\n",
        "        return self._get_observation()\n",
        "\n",
        "    def _get_observation(self):\n",
        "        return self.ant_position[0] * self.grid_size + self.ant_position[1]\n",
        "\n",
        "    def step(self, action):\n",
        "        if action == 0:  # Up\n",
        "            self.ant_position[0] = max(0, self.ant_position[0] - 1)\n",
        "        elif action == 1:  # Down\n",
        "            self.ant_position[0] = min(self.grid_size - 1, self.ant_position[0] + 1)\n",
        "        elif action == 2:  # Left\n",
        "            self.ant_position[1] = max(0, self.ant_position[1] - 1)\n",
        "        elif action == 3:  # Right\n",
        "            self.ant_position[1] = min(self.grid_size - 1, self.ant_position[1] + 1)\n",
        "\n",
        "        if self.ant_position == self.goal_position:\n",
        "            self.done = True\n",
        "            reward = 1\n",
        "        else:\n",
        "            reward = 0\n",
        "\n",
        "        return self._get_observation(), reward, self.done, {}\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, epsilon=0.1, alpha=0.5, gamma=0.99):\n",
        "        self.q_table = np.zeros((25, 4))  # Q-table for 5x5 grid world with 4 possible actions\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.randint(0, 3)  # Choose random action\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state])  # Choose action with highest Q-value\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state):\n",
        "        next_max = np.max(self.q_table[next_state])\n",
        "        self.q_table[state, action] += self.alpha * (reward + self.gamma * next_max - self.q_table[state, action])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = AntWorldEnv()\n",
        "    agent = QLearningAgent()\n",
        "\n",
        "    episodes = 10000\n",
        "    total_reward = 0\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.update_q_table(state, action, reward, next_state)\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "\n",
        "        total_reward += episode_reward\n",
        "\n",
        "        if episode % 500 == 0:\n",
        "            print(f\"Episode {episode}/{episodes}\")\n",
        "            print(\"Total Reward:\", total_reward)\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "    print(\"Q-Table:\")\n",
        "    print(agent.q_table)\n",
        "    print(\"Number of Episodes:\", episodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-p8A4IzagLp",
        "outputId": "354602ad-e261-418e-bcf5-153626cf64b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0/10000\n",
            "Total Reward: 1\n",
            "Episode 500/10000\n",
            "Total Reward: 501\n",
            "Episode 1000/10000\n",
            "Total Reward: 1001\n",
            "Episode 1500/10000\n",
            "Total Reward: 1501\n",
            "Episode 2000/10000\n",
            "Total Reward: 2001\n",
            "Episode 2500/10000\n",
            "Total Reward: 2501\n",
            "Episode 3000/10000\n",
            "Total Reward: 3001\n",
            "Episode 3500/10000\n",
            "Total Reward: 3501\n",
            "Episode 4000/10000\n",
            "Total Reward: 4001\n",
            "Episode 4500/10000\n",
            "Total Reward: 4501\n",
            "Episode 5000/10000\n",
            "Total Reward: 5001\n",
            "Episode 5500/10000\n",
            "Total Reward: 5501\n",
            "Episode 6000/10000\n",
            "Total Reward: 6001\n",
            "Episode 6500/10000\n",
            "Total Reward: 6501\n",
            "Episode 7000/10000\n",
            "Total Reward: 7001\n",
            "Episode 7500/10000\n",
            "Total Reward: 7501\n",
            "Episode 8000/10000\n",
            "Total Reward: 8001\n",
            "Episode 8500/10000\n",
            "Total Reward: 8501\n",
            "Episode 9000/10000\n",
            "Total Reward: 9001\n",
            "Episode 9500/10000\n",
            "Total Reward: 9501\n",
            "Training complete.\n",
            "Q-Table:\n",
            "[[0.92274469 0.93206535 0.92274469 0.93206535]\n",
            " [0.93206535 0.94148015 0.92274469 0.94148015]\n",
            " [0.94148015 0.95099005 0.93206535 0.95099005]\n",
            " [0.95099005 0.96059601 0.94148015 0.96059601]\n",
            " [0.96059601 0.970299   0.95099005 0.96059601]\n",
            " [0.92274469 0.92274469 0.93206535 0.94148015]\n",
            " [0.93206535 0.93206535 0.93206535 0.95099005]\n",
            " [0.94148015 0.96059601 0.94148015 0.96059601]\n",
            " [0.95099005 0.970299   0.95099005 0.970299  ]\n",
            " [0.96059601 0.9801     0.96059601 0.970299  ]\n",
            " [0.93206535 0.22837931 0.8074016  0.81434217]\n",
            " [0.94148015 0.79658819 0.46137235 0.480298  ]\n",
            " [0.95099005 0.94403392 0.92975046 0.970299  ]\n",
            " [0.96059601 0.9801     0.96059601 0.9801    ]\n",
            " [0.970299   0.99       0.970299   0.9801    ]\n",
            " [0.46137235 0.         0.         0.23068617]\n",
            " [0.93024491 0.         0.         0.        ]\n",
            " [0.96059601 0.42026075 0.82001726 0.94947187]\n",
            " [0.970299   0.970299   0.95099005 0.99      ]\n",
            " [0.9801     1.         0.9801     0.99      ]\n",
            " [0.22837931 0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.90965531]\n",
            " [0.9801     0.83999574 0.480298   0.75      ]\n",
            " [0.         0.         0.         0.        ]]\n",
            "Number of Episodes: 10000\n"
          ]
        }
      ]
    }
  ]
}