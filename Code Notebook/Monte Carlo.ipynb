{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXZkRfZDjdea",
        "outputId": "b590b73f-9ba9-4a7c-f5f2-328336dfcbc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S\t\n",
            "H\t\n",
            "H\t\n",
            "G\t\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class GridWorld:\n",
        "    def __init__(self, grid_size, start_state, goal_state, hole_states, actions):\n",
        "        self.grid_size = grid_size\n",
        "        self.start_state = start_state\n",
        "        self.goal_state = goal_state\n",
        "        self.hole_states = hole_states\n",
        "        self.actions = actions\n",
        "\n",
        "    def step(self, state, action):\n",
        "        next_state = np.array(state) + np.array(action)\n",
        "        next_state = np.clip(next_state, [0, 0], [self.grid_size[0]-1, self.grid_size[1]-1])  # Ensure the agent stays within the grid\n",
        "        if tuple(next_state) == self.goal_state:\n",
        "            reward = 1\n",
        "            done = True\n",
        "        elif tuple(next_state) in self.hole_states:\n",
        "            reward = -1\n",
        "            done = True\n",
        "        else:\n",
        "            reward = 0\n",
        "            done = False\n",
        "        return tuple(next_state), reward, done\n",
        "\n",
        "def monte_carlo_control(env, num_episodes=1000, epsilon=0.1, discount_factor=0.9):\n",
        "    Q = {}\n",
        "    N = {}  # Count of visits to state-action pairs\n",
        "    returns_sum = {}  # Sum of returns for state-action pairs\n",
        "\n",
        "    def policy(state):\n",
        "      actions_array = np.array(env.actions)  # Convert actions to numpy array\n",
        "      if np.random.rand() < epsilon:\n",
        "          return tuple(actions_array[np.random.choice(len(env.actions))])  # Randomly select an action\n",
        "      else:\n",
        "          return tuple(actions_array[np.argmax([Q.get((state, a), 0) for a in env.actions])])  # Choose action with maximum Q-value\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        episode = []\n",
        "        state = env.start_state\n",
        "        while True:\n",
        "            action = policy(state)\n",
        "            next_state, reward, done = env.step(state, action)\n",
        "            episode.append((state, action, reward))\n",
        "            if done:\n",
        "                break\n",
        "            state = next_state\n",
        "\n",
        "        G = 0\n",
        "        visited_state_actions = set()\n",
        "        for t in range(len(episode)-1, -1, -1):\n",
        "            state, action, reward = episode[t]\n",
        "            G = discount_factor * G + reward\n",
        "            sa_pair = (state, action)\n",
        "            if sa_pair not in visited_state_actions:\n",
        "                N[sa_pair] = N.get(sa_pair, 0) + 1\n",
        "                returns_sum[sa_pair] = returns_sum.get(sa_pair, 0) + G\n",
        "                Q[sa_pair] = returns_sum[sa_pair] / N[sa_pair]\n",
        "                visited_state_actions.add(sa_pair)\n",
        "\n",
        "    return Q\n",
        "\n",
        "# Define the grid world parameters\n",
        "grid_size = (4, 4)\n",
        "start_state = (0, 0)\n",
        "goal_state = (3, 3)\n",
        "hole_states = [(1, 1), (2, 2)]\n",
        "actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
        "\n",
        "# Create the grid world environment\n",
        "env = GridWorld(grid_size, start_state, goal_state, hole_states, actions)\n",
        "\n",
        "# Run Monte Carlo control to learn the optimal policy\n",
        "optimal_policy = monte_carlo_control(env)\n",
        "\n",
        "# Display the learned optimal policy\n",
        "for i in range(grid_size[0]):\n",
        "    for j in range(grid_size[1]):\n",
        "        state = (i, j)\n",
        "        if state == goal_state:\n",
        "            print(\"G\", end=\"\\t\")\n",
        "        elif state in hole_states:\n",
        "            print(\"H\", end=\"\\t\")\n",
        "        elif state == start_state:\n",
        "            print(\"S\", end=\"\\t\")\n",
        "        else:\n",
        "            action = optimal_policy.get(state, None)\n",
        "            if action == (0, 1):\n",
        "                print(\"→\", end=\"\\t\")\n",
        "            elif action == (0, -1):\n",
        "                print(\"←\", end=\"\\t\")\n",
        "            elif action == (1, 0):\n",
        "                print(\"↓\", end=\"\\t\")\n",
        "            elif action == (-1, 0):\n",
        "                print(\"↑\", end=\"\\t\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class GridWorld:\n",
        "    def __init__(self, grid_size, start_state, goal_state, hole_states, actions):\n",
        "        self.grid_size = grid_size\n",
        "        self.start_state = start_state\n",
        "        self.goal_state = goal_state\n",
        "        self.hole_states = hole_states\n",
        "        self.actions = actions\n",
        "\n",
        "    def step(self, state, action):\n",
        "        next_state = np.array(state) + np.array(action)\n",
        "        next_state = np.clip(next_state, [0, 0], [self.grid_size[0]-1, self.grid_size[1]-1])  # Ensure the agent stays within the grid\n",
        "        if tuple(next_state) == self.goal_state:\n",
        "            reward = 1\n",
        "            done = True\n",
        "        elif tuple(next_state) in self.hole_states:\n",
        "            reward = -1\n",
        "            done = True\n",
        "        else:\n",
        "            reward = 0\n",
        "            done = False\n",
        "        return tuple(next_state), reward, done\n",
        "\n",
        "def monte_carlo_control(env, num_episodes=1000, epsilon=0.1, discount_factor=0.9):\n",
        "    Q = {}\n",
        "    N = {}  # Count of visits to state-action pairs\n",
        "    returns_sum = {}  # Sum of returns for state-action pairs\n",
        "    episode_rewards = []\n",
        "\n",
        "    def policy(state):\n",
        "        actions_array = np.array(env.actions)\n",
        "        if np.random.rand() < epsilon:\n",
        "            return tuple(actions_array[np.random.choice(len(env.actions))])  # Randomly select an action\n",
        "        else:\n",
        "            return tuple(actions_array[np.argmax([Q.get((state, a), 0) for a in env.actions])])  # Choose action with maximum Q-value\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        episode = []\n",
        "        state = env.start_state\n",
        "        total_reward = 0\n",
        "        while True:\n",
        "            action = policy(state)\n",
        "            next_state, reward, done = env.step(state, action)\n",
        "            episode.append((state, action, reward))\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                episode_rewards.append(total_reward)\n",
        "                break\n",
        "            state = next_state\n",
        "\n",
        "        G = 0\n",
        "        visited_state_actions = set()\n",
        "        for t in range(len(episode)-1, -1, -1):\n",
        "            state, action, reward = episode[t]\n",
        "            G = discount_factor * G + reward\n",
        "            sa_pair = (state, action)\n",
        "            if sa_pair not in visited_state_actions:\n",
        "                N[sa_pair] = N.get(sa_pair, 0) + 1\n",
        "                returns_sum[sa_pair] = returns_sum.get(sa_pair, 0) + G\n",
        "                Q[sa_pair] = returns_sum[sa_pair] / N[sa_pair]\n",
        "                visited_state_actions.add(sa_pair)\n",
        "\n",
        "    # Calculate the average reward\n",
        "    avg_reward = np.mean(episode_rewards)\n",
        "\n",
        "    return Q, avg_reward\n",
        "\n",
        "def td_learning(env, num_episodes=1000, alpha=0.1, epsilon=0.1, discount_factor=0.9):\n",
        "    Q = {}\n",
        "    episode_rewards = []\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        state = env.start_state\n",
        "        total_reward = 0\n",
        "        while True:\n",
        "            if np.random.rand() < epsilon:\n",
        "                actions = [action[0] for action in env.actions]  # Extract actions from tuples\n",
        "                action = (np.random.choice(actions),)  # Randomly choose an action and convert it to tuple\n",
        "            else:\n",
        "                action_values = [Q.get((state, a), 0) for a, _ in env.actions]\n",
        "                action = env.actions[np.argmax(action_values)]\n",
        "\n",
        "            next_state, reward, done = env.step(state, action)\n",
        "            total_reward += reward\n",
        "\n",
        "            td_target = reward + discount_factor * max(Q.get((next_state, a), 0) for a, _ in env.actions)\n",
        "            td_error = td_target - Q.get((state, action), 0)\n",
        "            Q[(state, action)] = Q.get((state, action), 0) + alpha * td_error\n",
        "\n",
        "            if done:\n",
        "                episode_rewards.append(total_reward)\n",
        "                break\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "    # Calculate the average reward\n",
        "    avg_reward = np.mean(episode_rewards)\n",
        "\n",
        "    return Q, avg_reward\n",
        "\n",
        "# Define the grid world parameters\n",
        "grid_size = (4, 4)\n",
        "start_state = (0, 0)\n",
        "goal_state = (3, 3)\n",
        "hole_states = [(1, 1), (2, 2)]\n",
        "actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
        "\n",
        "# Create the grid world environment\n",
        "env = GridWorld(grid_size, start_state, goal_state, hole_states, actions)\n",
        "\n",
        "# Run Monte Carlo control to learn the optimal policy\n",
        "optimal_policy_mc, avg_reward_mc = monte_carlo_control(env)\n",
        "print(\"Monte Carlo Agent:\")\n",
        "print(\"Average Reward:\", avg_reward_mc)\n",
        "print(\"Learned State Values:\")\n",
        "for state in sorted(optimal_policy_mc.keys()):\n",
        "    print(f\"State: {state}, Value: {optimal_policy_mc[state]}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Run Temporal-Difference learning to learn the optimal policy\n",
        "optimal_policy_td, avg_reward_td = td_learning(env)\n",
        "print(\"Temporal-Difference Agent:\")\n",
        "print(\"Average Reward:\", avg_reward_td)\n",
        "print(\"Learned State Values:\")\n",
        "for state in sorted(optimal_policy_td.keys()):\n",
        "    print(f\"State: {state}, Value: {optimal_policy_td[state]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5HtgfnNju0U",
        "outputId": "8cd03181-1a84-4a30-c1e9-6264d0a655d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Monte Carlo Agent:\n",
            "Average Reward: 0.88\n",
            "Learned State Values:\n",
            "State: ((0, 0), (-1, 0)), Value: 0.3443947718251938\n",
            "State: ((0, 0), (0, -1)), Value: 0.4476613827318654\n",
            "State: ((0, 0), (0, 1)), Value: 0.49300303687387675\n",
            "State: ((0, 0), (1, 0)), Value: 0.23552073761512093\n",
            "State: ((0, 1), (-1, 0)), Value: 0.49093719995454566\n",
            "State: ((0, 1), (0, -1)), Value: 0.36923552619639627\n",
            "State: ((0, 1), (0, 1)), Value: 0.5941726054065737\n",
            "State: ((0, 1), (1, 0)), Value: -1.0\n",
            "State: ((0, 2), (-1, 0)), Value: 0.6341545862068968\n",
            "State: ((0, 2), (0, -1)), Value: 0.3004875684\n",
            "State: ((0, 2), (0, 1)), Value: 0.6757397684488391\n",
            "State: ((0, 2), (1, 0)), Value: 0.587438571428571\n",
            "State: ((0, 3), (-1, 0)), Value: 0.6984518157692308\n",
            "State: ((0, 3), (0, -1)), Value: 0.5676646263157896\n",
            "State: ((0, 3), (0, 1)), Value: 0.6120342865932273\n",
            "State: ((0, 3), (1, 0)), Value: 0.7592278475943711\n",
            "State: ((1, 0), (-1, 0)), Value: 0.5000990013045002\n",
            "State: ((1, 0), (0, -1)), Value: -0.9\n",
            "State: ((1, 0), (0, 1)), Value: -1.0\n",
            "State: ((1, 0), (1, 0)), Value: -0.40181865727943206\n",
            "State: ((1, 2), (-1, 0)), Value: 0.6561000000000001\n",
            "State: ((1, 2), (0, -1)), Value: -1.0\n",
            "State: ((1, 2), (0, 1)), Value: 0.7253872641511838\n",
            "State: ((1, 2), (1, 0)), Value: -1.0\n",
            "State: ((1, 3), (-1, 0)), Value: 0.7037453571428569\n",
            "State: ((1, 3), (0, -1)), Value: 0.622737864494781\n",
            "State: ((1, 3), (0, 1)), Value: 0.7988142857142859\n",
            "State: ((1, 3), (1, 0)), Value: 0.852906542056061\n",
            "State: ((2, 0), (0, -1)), Value: 0.6561000000000001\n",
            "State: ((2, 0), (0, 1)), Value: -0.4360935805278153\n",
            "State: ((2, 0), (1, 0)), Value: 0.7290000000000001\n",
            "State: ((2, 1), (0, -1)), Value: 0.030903154382632643\n",
            "State: ((2, 1), (0, 1)), Value: -1.0\n",
            "State: ((2, 3), (-1, 0)), Value: 0.81\n",
            "State: ((2, 3), (0, -1)), Value: -1.0\n",
            "State: ((2, 3), (0, 1)), Value: 0.8279999999999998\n",
            "State: ((2, 3), (1, 0)), Value: 1.0\n",
            "State: ((3, 0), (0, 1)), Value: 0.81\n",
            "State: ((3, 1), (0, 1)), Value: 0.9\n",
            "State: ((3, 2), (0, 1)), Value: 1.0\n",
            "\n",
            "\n",
            "Temporal-Difference Agent:\n",
            "Average Reward: 0.934\n",
            "Learned State Values:\n",
            "State: ((0, 0), (-1,)), Value: 0.0\n",
            "State: ((0, 0), (0,)), Value: 0.0\n",
            "State: ((0, 0), (0, 1)), Value: 0.0\n",
            "State: ((0, 0), (1,)), Value: -0.9690968456173674\n",
            "State: ((0, 1), (-1,)), Value: 0.0\n",
            "State: ((0, 1), (0,)), Value: 0.0\n",
            "State: ((0, 1), (0, 1)), Value: 0.0\n",
            "State: ((0, 1), (1,)), Value: 0.0\n",
            "State: ((0, 2), (-1,)), Value: 0.0\n",
            "State: ((0, 2), (0,)), Value: 0.0\n",
            "State: ((0, 2), (0, 1)), Value: 0.0\n",
            "State: ((0, 2), (1,)), Value: 0.0\n",
            "State: ((0, 3), (-1,)), Value: 0.0\n",
            "State: ((0, 3), (0,)), Value: 0.0\n",
            "State: ((0, 3), (0, 1)), Value: 0.0\n",
            "State: ((0, 3), (1,)), Value: 0.0\n",
            "State: ((1, 2), (-1,)), Value: 0.0\n",
            "State: ((1, 2), (0,)), Value: 0.0\n",
            "State: ((1, 2), (0, 1)), Value: 0.0\n",
            "State: ((1, 2), (1,)), Value: 0.0\n",
            "State: ((1, 3), (-1,)), Value: 0.0\n",
            "State: ((1, 3), (0,)), Value: 0.0\n",
            "State: ((1, 3), (0, 1)), Value: 0.0\n",
            "State: ((1, 3), (1,)), Value: 0.0\n",
            "State: ((2, 3), (-1,)), Value: 0.0\n",
            "State: ((2, 3), (0,)), Value: 0.0\n",
            "State: ((2, 3), (0, 1)), Value: 0.0\n",
            "State: ((2, 3), (1,)), Value: 0.9999999999999996\n"
          ]
        }
      ]
    }
  ]
}