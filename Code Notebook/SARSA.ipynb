{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**NAME**: RUPESH DHIRWANI\n",
        "\n",
        "**CLASS:** D16AD\n",
        "\n",
        "**ROLL NO:** 10"
      ],
      "metadata": {
        "id": "cqRYXzXEw7qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "grid_size = 5\n",
        "start_state = (0, 0)\n",
        "goal_state = (grid_size - 1, grid_size - 1)\n",
        "obstacles = [(1, 1), (2, 2), (3, 1)]\n",
        "\n",
        "actions = ['up', 'down', 'left', 'right']\n",
        "num_actions = len(actions)\n",
        "\n",
        "learning_rate = 0.1\n",
        "discount_factor = 0.9\n",
        "epsilon = 0.1\n",
        "num_episodes = 1500\n",
        "\n",
        "# Define Antworld environment\n",
        "class Antworld:\n",
        "    def __init__(self, grid_size, obstacles, start_state, goal_state):\n",
        "        self.grid_size = grid_size\n",
        "        self.obstacles = obstacles\n",
        "        self.start_state = start_state\n",
        "        self.goal_state = goal_state\n",
        "\n",
        "    def reset(self):\n",
        "        return self.start_state\n",
        "\n",
        "    def step(self, state, action):\n",
        "        if state == self.goal_state:\n",
        "            return state, 0\n",
        "\n",
        "        next_state = self.get_next_state(state, action)\n",
        "        if next_state in self.obstacles:\n",
        "            return state, -1\n",
        "        elif next_state == self.goal_state:\n",
        "            return next_state, 1\n",
        "        else:\n",
        "            return next_state, 0\n",
        "\n",
        "    def get_next_state(self, state, action):\n",
        "        x, y = state\n",
        "        if action == 'up':\n",
        "            return max(x - 1, 0), y\n",
        "        elif action == 'down':\n",
        "            return min(x + 1, self.grid_size - 1), y\n",
        "        elif action == 'left':\n",
        "            return x, max(y - 1, 0)\n",
        "        elif action == 'right':\n",
        "            return x, min(y + 1, self.grid_size - 1)\n",
        "\n",
        "# Initialize Antworld environment\n",
        "antworld_env = Antworld(grid_size, obstacles, start_state, goal_state)\n",
        "\n",
        "Q_values = np.zeros((grid_size, grid_size, num_actions))\n",
        "\n",
        "def select_action(state):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.choice(actions)\n",
        "    else:\n",
        "        return actions[np.argmax(Q_values[state])]\n",
        "\n",
        "def update_Q_value(state, action, reward, next_state, next_action):\n",
        "    next_action_index = actions.index(next_action)\n",
        "    Q_values[state][actions.index(action)] += learning_rate * (reward + discount_factor * Q_values[next_state][next_action_index] - Q_values[state][actions.index(action)])\n",
        "\n",
        "#SARSA Training\n",
        "for episode in range(num_episodes):\n",
        "    state = antworld_env.reset()\n",
        "    action = select_action(state)\n",
        "    while state != goal_state:\n",
        "        next_state, reward = antworld_env.step(state, action) # corrected line\n",
        "        next_action = select_action(next_state)\n",
        "        update_Q_value(state, action, reward, next_state, next_action)\n",
        "        state = next_state\n",
        "        action = next_action\n",
        "\n",
        "print(\"Learned Q - values:\")\n",
        "print(Q_values)\n",
        "\n",
        "path = []\n",
        "\n",
        "state = start_state\n",
        "while state != goal_state:\n",
        "    action = actions[np.argmax(Q_values[state])]\n",
        "    path.append((state, action))\n",
        "    state = antworld_env.get_next_state(state, action)\n",
        "path.append((goal_state, None))\n",
        "\n",
        "print(\"\\nShortest Path:\")\n",
        "for step in path:\n",
        "    print(step)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J0zm4dywVDt",
        "outputId": "8f472237-08df-4371-9bb2-d3947a652def"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned Q - values:\n",
            "[[[ 2.65339579e-01  2.31621820e-01  2.82591384e-01  4.00773064e-01]\n",
            "  [ 3.73860920e-01 -6.13071881e-01  3.01308184e-01  4.51274829e-01]\n",
            "  [ 4.38932594e-01  3.02270921e-01  3.29012314e-01  5.21699354e-01]\n",
            "  [ 4.98550798e-01  5.76033887e-01  3.90948039e-01  4.09287120e-01]\n",
            "  [ 3.27144907e-02  3.11729709e-02  4.91921198e-01  1.05200065e-01]]\n",
            "\n",
            " [[ 2.55714890e-01 -1.40022968e-04  2.74076633e-02 -4.01909711e-01]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            "  [ 4.34168105e-01 -3.55910145e-01 -1.70776816e-01  1.06131761e-01]\n",
            "  [ 4.58516674e-01  6.83628257e-01  3.68872346e-01  2.74711921e-01]\n",
            "  [ 3.77917504e-01  5.75486648e-02  0.00000000e+00  0.00000000e+00]]\n",
            "\n",
            " [[-1.91532881e-06 -1.38541773e-03 -1.53929719e-03 -1.53900003e-02]\n",
            "  [-1.90000000e-01 -2.71000000e-01 -3.90907273e-07 -1.90000000e-01]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            "  [ 5.67332887e-01  7.79445638e-01 -3.08924380e-01  5.19550763e-01]\n",
            "  [ 4.31263595e-03  9.97440572e-03  6.56887390e-01  3.24166889e-06]]\n",
            "\n",
            " [[-6.31024848e-07 -5.31441000e-06 -1.53900490e-02 -2.71000048e-01]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            "  [-2.71000000e-01  7.17499980e-01 -4.41858557e-02  0.00000000e+00]\n",
            "  [ 6.16327510e-01  8.77290211e-01  4.89696703e-01  3.34253384e-01]\n",
            "  [ 5.25239367e-01  1.90000000e-01  7.75532840e-02  0.00000000e+00]]\n",
            "\n",
            " [[-7.29000000e-03 -7.29000000e-04 -7.44495697e-05 -1.01452087e-04]\n",
            "  [-2.71000000e-01 -9.00000000e-03 -6.02365410e-04  1.60510093e-01]\n",
            "  [ 2.74619583e-02  1.62196662e-01 -2.53114604e-03  8.97563144e-01]\n",
            "  [ 7.46154943e-01  8.59184353e-01  7.05566409e-01  1.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]]\n",
            "\n",
            "Shortest Path:\n",
            "((0, 0), 'right')\n",
            "((0, 1), 'right')\n",
            "((0, 2), 'right')\n",
            "((0, 3), 'down')\n",
            "((1, 3), 'down')\n",
            "((2, 3), 'down')\n",
            "((3, 3), 'down')\n",
            "((4, 3), 'right')\n",
            "((4, 4), None)\n"
          ]
        }
      ]
    }
  ]
}